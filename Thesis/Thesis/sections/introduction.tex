% section
\section{Introduction} \label{section::introduction}
 The task of this chapter is to give the reader the basic knowledge, which is necessary to follow the rest of the thesis. In general, the reader should have basic knowledge about machine learning
 and neural networks.
 As this thesis is in the field of machine learning, more explicit neural networks and image-/video-prediction, this chapter will start giving basic knowledge about image-/video-prediction
 and specific neural network architectures, which are used in the thesis.\\\\
 The necessary neural network architectures are Autoencoder~\ref{subsection::autoencoder}, RNN~\ref{subsection::rnn}, LSTM~\ref{subsection::lstm}
 and ConvLSTM~\ref{subsection::convlstm}, they are described briefly for the reader.
 Afterwards I describe the backpropagation algorithm~\ref{subsection::backpropagation} and BPTT~\ref{subsection::bptt} at a very basic level.
 Those two algorithms are the most used algorithms in training neural networks and are used in this thesis. Lastly I describe PyTorch~\ref{subsection::pytorch}, in which I re-implemented the baselines
 for this thesis.

 % subsection
 \subsection{Image Prediction / Video Prediction} \label{subsection::imageprediction}
  Image-/Video-prediction is a field inside machine learning, where the key is to predict future images, given a sequence of images. The image sequence $X$ is of length $n$, ($x_0, \ldots, x_{n-1}$).
  \\\\
  One possible use-case is the \textbf{one-frame prediction}, where one predicts $x_n$, given the the sequence $X$. Another common use-case is \textbf{multi-frame prediction}, where the key is to 
  predict $t > 1$ many frames into the future ($x_n, \ldots, x_{n+t-1}$).
  This is often performed using sequence-to-sequence learning \cite{Sutskever2014}. The first frames will look much better then the last frames, as 
  ground-truth is missing, The predicted frames are only approximated, which means they contain a certain level of error, so using them as input to perform \textbf{multi-frame prediction} will
  increase the level of error for the following frames.

 % subsection
 \subsection{Autoencoder} \label{subsection::autoencoder}
  The autoencoder is a network architecture, which consists of two neural networks chained together. The first network is called Encoder. This Encoder gets the input $x$ and outputs
  the code h. Often the output layer of the Encoder is named bottleneck-layer. The second network is called Decoder. It gets the code $h$ as input
  and outputs $x \prime$. This architecture is used for reconstruction, where $x \approx x \prime$. To prevent the architecture to simply copy the input directly to the output (which would be an
  interpolation and not the goal of any machine learning algorithm.), there are different techniques to have the autoencoder to instead approximate the output.\\\\
  \begin{equation}
   E(x) = h
  \end{equation}
  \begin{equation}
   D(h) = x \prime
  \end{equation}
  \begin{equation}
   D(E(x)) = x \prime
  \end{equation}
  The simplest autoencoder architecture is the so named undercomplete autoencoder \cite{Goodfellow2016}, in which the output of the bottleneck-layer $h$ is smaller then the input $x$.
  Therefore the architecture needs to learn how to extract useful features from the input $x$, because it is not able to simply copy the input $x$ to the output $x \prime$, because $h$ is a reduced
  representation of $x$. There are many different ideas of using the autoencoder architecture, which are described more in-depth in e.g. Goodfellow et. al. \cite{Goodfellow2016}.
  \begin{figure}[H]
   \includegraphics[width=0.35\textwidth]{../Images/autoencoder_schema.png}
   \centering
   \caption{Autoencoder schema \cite{wiki2019}}
   \label{fig:lstm_architecture}
  \end{figure}

 % subsection
 \subsection{RNN} \label{subsection::rnn}
  RNN (Recurrent neural network) is a network type, which is able to handle sequential data $X = (x_0, \ldots, x_{t-1})$, $|X| = t$. Therefore this type of network is used for e.g. time-series 
  analysis and image-/video-prediction~\ref{subsection::imageprediction}.\\\\
  Despite a standard feed-forward neural network, a RNN will not only get a new input at a time-step, but also the output
  of the RNN of the last time-step. This requires a RNN to be initialized at start, because there is no output of the last time-step available. This last time-step input is often initialized as $0$.
  A standard feed-forward neural network looks like:
  \begin{equation}
   \hat{y} = f_{\theta}(x)
  \end{equation}
  Every approximated output $\hat{y}$ is only dependent of the input $x$ and the computation inside the network.
  The RNN looks like:
  \begin{equation}
   \hat{y^t} = f_{\theta}(\hat{y^{t-1}};x^t) = f_{\theta}(f_{\theta}(\hat{y^{t-2}};x^{t-1});x^t) = \ldots
  \end{equation}
  The approximated output $\hat{y^t}$ depends on the input $x^t$, but also on all previous outputs.
  In the literature the RNN is often schemed using a folded and an unfolded graph, to illustrate how the network architecture works.
  \begin{figure}[H]
   \includegraphics[width=0.8\textwidth]{../Images/rnn.png}
   \centering
   \caption{RNN schema \cite{Olah2015}}
   \label{fig:lstm_architecture}
  \end{figure}\noindent
  The output of a RNN is often denoted as $h$ for hidden-unit, because it is not only the output of the time-step, but also the input for the next time-step. \label{sentence::hidden}\\
  This networks are not learned with simple backpropagation~\ref{subsection::backpropagation}, but often with BPTT (Backpropagation through time)~\ref{subsection::bptt}.
 
 % subsection
 \subsection{LSTM} \label{subsection::lstm}
  LSTM (Long Short-term Memory), invented by Hochreiter and Schmidhuber \cite{Hochreiter1997} is a form of RNN, which avoids a critical problem of standard RNN: Saving 
  \textbf{Long-term dependencies} 
  \cite{Goodfellow2016}.
  The architecture consists of different submodules: An inpute-gate, forget-gate, cell-state and output-gate.
  \begin{equation}
   i_t = \sigma(w_{x_i}x_t + w_{h_i}h_{t-1} + b_i)
  \end{equation}
  \begin{equation}
   f_t = \sigma(w_{x_f}x_t + w_{h_f}h_{t-1} + b_f)
  \end{equation}
  \begin{equation}
   c_t = f_tc_{t-1} + i_ttanh(w_{x_c}x_t + w_{h_c}h_{t-1} + b_c)
  \end{equation}
  \begin{equation}
   o_t = \sigma(w_{x_o}x_t + w_{h_o}h_{t-1} + b_o)
  \end{equation}
  \begin{equation}
   h_t = o_ttanh(c_t)
  \end{equation}
  $w$ is the weight of the layer\\
  $\sigma$ the sigmoid function\\
  $b$ the layer bias.\\
  $h_t$ is the output, in RNN's the output is often denoted as hidden~\ref{sentence::hidden}.
  \begin{figure}[H]
   \includegraphics[width=0.7\textwidth]{../Images/lstm_chain.png}
   \centering
   \caption{LSTM Architecture \cite{Olah2015}}
   \label{fig:lstm_architecture}
  \end{figure}\noindent
  The given equations for the LSTM are for the very basic LSTM without peephole.\\\\
  The peephole is an idea from Gers and Schmidhuber from the year 2000 \cite{Gers2000}, where they augmented the LSTM
  with peephole connections, which gave them an advantage in learning spike trains$^{\ref{bottom::spiktetrain}}$, because a LSTM without peephole was not able to learn those spike trains.
  The equations looks very similar, with only few changes:
  \begin{equation}
   i_t = \sigma(w_{x_i}x_t + w_{c_i}c_{t-1} + b_i)
  \end{equation}
  \begin{equation}
   f_t = \sigma(w_{x_f}x_t + w_{c_f}c_{t-1} + b_f)
  \end{equation}
  \begin{equation}
   c_t = f_tc_{t-1} + i_ttanh(w_{x_c}x_t + b_c)
  \end{equation}
  \begin{equation}
   o_t = \sigma(w_{x_o}x_t + w_{c_o}c_t + b_o)
  \end{equation}
  \begin{equation}
   h_t = o_ttanh(c_t)
  \end{equation}
  There are of course many other implementations, but those two are the most basic ones and typically used as baseline.\\
  %\vfill
  \noindent\rule{\textwidth}{1pt}
  \label{bottom::spiktetrain} Spike trains are time series of $0$s and $1$s, where $1$ is defined as a spike.
 % subsection
 \subsection{ConvLSTM} \label{subsection::convlstm}
  The convolutional LSTM, invented by Shi et. al. \cite{Shi2015} is a LSTM with peephole using convolutional layer instead of fully connected ones.
  Therefore the formulas look very similar to the ones in section~\ref{subsection::lstm}.
  \begin{equation}
   i_t = \sigma(x_t \ast w_{x_i} + h_{t-1} \ast w_{h_i} + w_{i_b})
  \end{equation}
  \begin{equation}
   f_t = \sigma(x_t \ast w_{x_f} + h_{t-1} \ast w_{h_f} + w_{f_b})
  \end{equation}
  \begin{equation}
   \tilde{c_t} = tanh(x_t \ast w_{x_{\tilde{c}}} + h_{t-1} \ast w_{h_{\tilde{c}}} + w_{c_{\tilde{b}}})
  \end{equation}
  \begin{equation}
   c_t = \tilde{c_t} \odot i_t + c_{t-1} \odot f_t
  \end{equation}
  \begin{equation}
   o_t = \sigma(x_t \ast w_{x_o} + h_{t-1} \ast w_{h_o} + w_{o_b})
  \end{equation}
  \begin{equation}
   h_t = o_t \odot tanh(c_t)
  \end{equation}
  $\ast$ is the commonly used sign for the convolution operation.\\
  $\odot$ is the hadamard product (point-wise multiplication).\\\\
  There is also a ConvLSTM without peephole, which is used in Patraucean et. al. \cite{Patraucean2015}, which is implemented in section~\ref{section::implementation}.
  
 % subsection
 \subsection{Backpropagation} \label{subsection::backpropagation}
  Backpropagation should be already known to the reader, therefore I will only give a very simple example of it at the most simple neural network, the Perceptron \cite{Rosenblatt1957}.
  \begin{center}
   \begin{tikzpicture}[roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=7mm},
  					 roundnodesmall/.style={circle, draw=green!60, fill=green!5, thick, minimum size=1mm}]
    % Node
    \node[roundnode] (circle) {$\sum | \sigma$};
    \node[roundnodesmall] (x1) [above left=of circle] {$x_1$};
    \node[roundnodesmall] (x2) [below left=of circle] {$x_2$};
    \node[roundnodesmall] (y) [right=of circle] {$\hat{y}$};
    % Lines
    \draw[->] (x1.east) -- (circle.west) node[left,above] {$w_1$};
    \draw[->] (x2.east) -- (circle.west) node[left,below] {$w_2$};
    \draw[->] (circle.east) -- (y);
   \end{tikzpicture}
  \end{center}
  The usage of backpropagation is very straight forward. Neural networks are representation learning algorithms, which means, that they learn the representation of the data over time, without
  having the need of an expert doing pre-processing of anything. It only needs a valid training and testing dataset, where have ground-truth knowledge of the output of the data. One then leverages
  the forward pass of the algorithm to produce our approximated output.
  % todo: fix tikz graph
  \textbf{Forward pass:}
  \begin{equation}
   \hat{y} = \sigma(\sum_{i=1}^{2}x_iw_i)
  \end{equation}
  After computing the regarding output, one will compare the computed output with the ground-truth output with some kind of error-function, e.g. \textbf{MSE}:
  \begin{equation}
   L(y,\hat{y}) = \frac{1}{N}||y-\hat{y}||_2^2 = \frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2
  \end{equation}    
  This error is then propagated back, using the chain-rule, through the
  graph to update the weights of the network. This is done in the backward pass.\\
  \textbf{Backward pass:}
  \begin{equation}
   \frac{\partial L}{\partial \hat{y}} = \frac{2}{N}\sum_{i=1}^{N}(y_i-\hat{y}_i)
  \end{equation}
  \begin{equation}
   \frac{\partial \sigma(x)}{\partial x} = \sigma(x)(1-\sigma(x))
  \end{equation}
  \begin{equation}
   \frac{\partial L}{\partial \sum_{i=1}^{2}x_iw_i} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial \sum_{i=1}^{2}x_iw_i} = \frac{2}{N}\sum_{i=1}^{N}(y_i-\hat{y}_i) \cdot
   \sigma(\sum_{i=1}^{2}x_iw_i)(1-\sigma(\sum_{i=1}^{2}x_iw_i))
  \end{equation}
 
 % subsection
 \subsection{Backpropagation through time (BPTT)} \label{subsection::bptt}
  
 % subsection
 \subsection{PyTorch} \label{subsection::pytorch}